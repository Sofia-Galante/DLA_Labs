{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f5d0b9d-7980-4d2c-8154-c07a5f8b5525",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <font color='red'>Introduction\n",
    "<font color='red'>In this laboratory we will get our hands dirty working with Large Language Models (e.g. GPT and BERT) to do various useful things. I you haven't already, it is highly recommended to:\n",
    "\n",
    "+ <font color='red'>Read the [Attention is All you Need](https://arxiv.org/abs/1706.03762) paper, which is the basis for all transformer-based LLMs.\n",
    "+ <font color='red'>Watch (and potentially *code along*) with this [Andrej Karpathy video](https://www.youtube.com/watch?v=kCc8FmEb1nY) which shows you how to build an autoregressive GPT model from the ground up.\n",
    "\n",
    "# <font color='red'>Exercise 1: Warming Up\n",
    "<font color='red'>In this first exercise you will train a *small* autoregressive GPT model for character generation (the one used by Karpathy in his video) to generate text in the style of Dante Aligheri. Use [this file](https://archive.org/stream/ladivinacommedia00997gut/1ddcd09.txt), which contains the entire text of Dante's Inferno (**note**: you will have to delete some introductory text at the top of the file before training). Train the model for a few epochs, monitor the loss, and generate some text at the end of training. Qualitatively evaluate the results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0ef17c",
   "metadata": {},
   "source": [
    "Il codice sotto riportato è stato ripreso dal [github di Karpathy](https://github.com/karpathy/ng-video-lecture) e riadattato all'interno del notebook. Il testo di Dante Alighieri è nel file *Dante.txt*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35568dbe-c37d-4d4a-834e-fb6d401c3c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793aa894-0a71-42e3-a418-726f9f8229e0",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Il modello qui sotto riportato è quello mostrato da Karpathy nel suo video. Non ci sono differenze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5737b2d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d1d5f3-eb77-4bee-8b66-5420c278ee1b",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "\n",
    "Si noti che questa parte del notebook è stata svolta sempre seguendo il video di Karpathy e che quindi ci saranno molte somiglianze con esso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86aa9248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1fe4e75d650>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iperparametri (sono gli stessi scelti da Karpathy)\n",
    "batch_size = 64\n",
    "block_size = 256\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ebf7095-1f6a-4a10-a3ba-1569a074fa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(string):\n",
    "    return [char_to_int[ch] for ch in string]\n",
    "\n",
    "def decode(integers):\n",
    "    return[int_to_char[i] for i in integers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50420b84-ab35-43c7-a486-408e7cf34058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apriamo il file\n",
    "with open('Dante.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Prendiamo i caratteri unici nel testo\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Associamo a ogni carattere un numero (e viceversa)\n",
    "char_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "int_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0ab63c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3c1a8e-123a-40ad-81de-6cb396b28545",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training and Experiments\n",
    "\n",
    "La funzione di training riportata di seguito è ispirata al ciclo di training di Karpathy, ma ho apportato alcune modifiche per poter monitorare meglio la loss e salvare i modelli a ogni valutazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "279e2dc6-e4a3-4c3e-a299-fd52c1d5bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dante(model):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    train_losses = list()\n",
    "    val_losses = list()\n",
    "    for i in range(max_iters):\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train')\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if (i+1) % eval_interval == 0 or (i+1) == max_iters:\n",
    "            losses = estimate_loss()\n",
    "            torch.save(model, f'dante-{i+1}.pth')\n",
    "            train_losses.append(losses['train'])\n",
    "            val_losses.append(losses['val'])\n",
    "            print(f\"step {i+1}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca5b52f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 300: train loss 2.1541, val loss 2.1841\n",
      "step 600: train loss 1.6408, val loss 1.7047\n",
      "step 900: train loss 1.3372, val loss 1.5493\n",
      "step 1200: train loss 1.0479, val loss 1.5421\n",
      "step 1500: train loss 0.7245, val loss 1.7091\n",
      "step 1800: train loss 0.4121, val loss 1.9432\n",
      "step 2100: train loss 0.2225, val loss 2.2876\n",
      "step 2400: train loss 0.1467, val loss 2.5798\n",
      "step 2700: train loss 0.1140, val loss 2.8148\n",
      "step 3000: train loss 0.1010, val loss 2.9315\n"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel().to(device)\n",
    "train_losses, val_losses = train_dante(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8a711a-b2cc-4706-a413-9df63f1a40ef",
   "metadata": {},
   "source": [
    "Ecco il grafico del training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02e027b0-6b6c-4e5c-b8e9-5a3f3b538494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#Funzione per il plot del grafico\n",
    "def plot(x, datas, labels, x_label, y_label, title):\n",
    "    for i in range(len(datas)):\n",
    "        plt.plot(x, datas[i], label = labels[i])\n",
    "    plt.legend()\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4791330-79b3-43b0-abbf-6e30c6f04d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 300: train loss 2.1541, val loss 2.1841\n",
      "step 600: train loss 1.6408, val loss 1.7047\n",
      "step 900: train loss 1.3372, val loss 1.5493\n",
      "step 1200: train loss 1.0479, val loss 1.5421\n",
      "step 1500: train loss 0.7245, val loss 1.7091\n",
      "step 1800: train loss 0.4121, val loss 1.9432\n",
      "step 2100: train loss 0.2225, val loss 2.2876\n",
      "step 2400: train loss 0.1467, val loss 2.5798\n",
      "step 2700: train loss 0.1140, val loss 2.8148\n",
      "step 3000: train loss 0.1010, val loss 2.9315\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot\n",
    "\n",
    "x = (np.arange(len(train_losses))+1)*eval_interval\n",
    "\n",
    "plot(\n",
    "    x,\n",
    "    (train_losses, val_losses),\n",
    "    ('Train', 'Validation'),\n",
    "    'Step',\n",
    "    'Loss',\n",
    "    'Training on Dante\\'s text'\n",
    ")\n",
    "\n",
    "for i in range(len(x)):\n",
    "    print(f\"step {x[i]}: train loss {train_losses[i]:.4f}, val loss {val_losses[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f81e518-0b83-49ca-84b7-bb91452e8e4d",
   "metadata": {},
   "source": [
    "Osservando il plot della loss e il log del training, si nota che il modello va in overfitting dopo 1200 iterazioni.\n",
    "Di seguito si caricano il modello migliore (1200 iterazioni) e il modello più overfittato (3000 iterazioni) per fare un confronto diretto sulla generazione del testo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2755df83-b32d-462f-8d1b-a2324598d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44df5a20-eae1-4761-834f-474190ea9454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  quando Francescando spia non di lor vastro>>.\n",
      "\n",
      "Poi cominciasce dorte dal monde manto,\n",
      "  <<S'ei or de` la vita tutta morta!\n",
      "  Verolte tenese ch'un poco unzo\n",
      "  la test'a dir priva 'l sabbion d'inferse,\n",
      "\n",
      "dal cinque che disperanza non e` daggire,\n",
      "  li Falchi suo che l'abbianco non s'anni.\n",
      "\n",
      "Gia` eravam la lunga conda gentero\n",
      "  che si` crucciato suo l'acqua comero\n",
      "  che 'l faera a le prime piu` percosso cse.\n",
      "\n",
      "Elle rilor de la tua proda ad Atte;\n",
      "  per che, se sono ricorto` quell'anima,\n",
      "  tu non piego\n"
     ]
    }
   ],
   "source": [
    "model_3000 = torch.load('dante-3000.pth')\n",
    "print(''.join(decode(model.generate(context, max_new_tokens=500)[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c72e0ba9-b3a4-41d8-904f-62bec6d7e43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ae di Polor danno, e l'altro e lor vaso;\n",
      "  cadde giuso in su la ripa debia,\n",
      "  e disse a Nesso: <<Or sier Bruno\n",
      "\n",
      "lo tuo nome tornasse e rinasciati,\n",
      "  a Minos a cor sie vissi  potea lascia,\n",
      "  anda veder lo sentiro piu` piano,\n",
      "\n",
      "quando incontanento a la li altri appassi,\n",
      "  che 'ntrammo l'un rombe e l'armane;\n",
      "  per che 'l fosso si riparea la faccia,\n",
      "\n",
      "non che di pina, sua si saetta persa,\n",
      "  mo tutto le folto infino e la chiasche,\n",
      "  poi ser tra 'mpie in torna si` fresta,\n",
      "  ch'alcun remar di terra a pro\n"
     ]
    }
   ],
   "source": [
    "model_1200 = torch.load('dante-1200.pth')\n",
    "print(''.join(decode(model.generate(context, max_new_tokens=500)[0].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a04116-4543-4510-bf2e-1c5562d7965f",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Osservando i due testi si nota come nessuno dei due sia in grado di riprodurre un testo con un vero e proprio significato. Evidentemente un modello generativo che predice in base ai caratteri non è abbastanza potente per un task simile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68441a09-dfaf-424a-b640-4fc8cea289b5",
   "metadata": {},
   "source": [
    "# <font color='red'>Exercise 2: Working with Real LLMs\n",
    "\n",
    "<font color='red'>Our toy GPT can only take us so far. In this exercise we will see how to use the [Hugging Face](https://huggingface.co/) model and dataset ecosystem to access a *huge* variety of pre-trained transformer models.\n",
    "\n",
    "## <font color='red'>Exercise 2.1: Installation and text tokenization\n",
    "\n",
    "<font color='red'>First things first, we need to install the [Hugging Face transformer library](https://huggingface.co/docs/transformers/index):\n",
    "\n",
    "    conda install -c huggingface -c conda-forge transformers\n",
    "    \n",
    "<font color='red'>The key classes that you will work with are `GPT2Tokenizer` to encode text into sub-word tokens, and the `GPT2LMHeadModel`. **Note** the `LMHead` part of the class name -- this is the version of the GPT2 architecture that has the text prediction heads attached to the final hidden layer representations (i.e. what we need to **generate** text). \n",
    "\n",
    "<font color='red'>Instantiate the `GPT2Tokenizer` and experiment with encoding text into integer tokens. Compare the length of input with the encoded sequence length.\n",
    "\n",
    "<font color='red'>**Tip**: Pass the `return_tensors='pt'` argument to the togenizer to get Pytorch tensors as output (instead of lists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af199a6d-1f3a-4b2c-a23f-d697b93c5adb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ba571e9-7a02-4ca9-84d5-615b0eb617e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2437,  389,  345,   30]])\n",
      "tensor([[3237,  318,  880,   11, 5875,  345,    0]])\n",
      "tensor([[15496,   995,    13]])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(\"How are you?\", return_tensors='pt')['input_ids'])\n",
    "print(tokenizer(\"All is well, thank you!\", return_tensors='pt')['input_ids'])\n",
    "print(tokenizer(\"Hello world.\", return_tensors='pt')['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b486dbc",
   "metadata": {},
   "source": [
    "Come si può notare, il modello sopra riportato crea un token diverso per ogni parola e simbolo di punteggiatura all'interno della frase. I token cambiano a seconda dell'utilizzo di lettere maiuscole o minuscole:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f79d9236-d1f5-4d16-81bb-4ca15139685c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15496,   995]])\n",
      "tensor([[15496,  2159]])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(\"Hello world\", return_tensors='pt')['input_ids'])\n",
    "print(tokenizer(\"Hello World\", return_tensors='pt')['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157a859a-4ef5-4b05-a6ce-258865ccc7e6",
   "metadata": {},
   "source": [
    " I segni di punteggiatura sono dei token a parte, infatti se si prende la stessa frase con e senza punteggiatura, si nota che vengono gli stessi token a parte per i segni di punteggiatura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18c4d00e-70e0-485f-8b05-5bc05733df04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15496,   995]])\n",
      "tensor([[15496,   995,     0]])\n",
      "tensor([[15496,   995,    13]])\n",
      "tensor([[15496,   995,    30]])\n",
      "tensor([[15496,     0,   995,     0]])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(\"Hello world\", return_tensors='pt')['input_ids'])\n",
    "print(tokenizer(\"Hello world!\", return_tensors='pt')['input_ids'])\n",
    "print(tokenizer(\"Hello world.\", return_tensors='pt')['input_ids'])\n",
    "print(tokenizer(\"Hello world?\", return_tensors='pt')['input_ids'])\n",
    "print(tokenizer(\"Hello! world!\", return_tensors='pt')['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e275d666-a5fd-485d-9391-fb8bb7f8b954",
   "metadata": {},
   "source": [
    "Inoltre se si combinano segni di punteggiatura diversi, si otterrà un token diverso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b31675ff-bd48-4e08-8e1b-5ee55e68c160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15496,   995,     0]])\n",
      "tensor([[15496,   995,    30]])\n",
      "tensor([[15496,   995, 12248]])\n",
      "tensor([[15496,   995, 22857]])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(\"Hello world!\", return_tensors='pt')['input_ids'])\n",
    "print(tokenizer(\"Hello world?\", return_tensors='pt')['input_ids'])\n",
    "print(tokenizer(\"Hello world?!\", return_tensors='pt')['input_ids'])\n",
    "print(tokenizer(\"Hello world!?\", return_tensors='pt')['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e754076d-ae8e-4602-bb27-8d29f973c046",
   "metadata": {},
   "source": [
    "Infine, il tokenizer è settato sulla lingua inglese. Infatti, se si prova a scrivere una frase in un'altra lingua il numero di token non corrisponde al numero di parole:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3afe7bfd-74b8-43bd-b485-f7b9d44bac1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   34, 13481,    11,  1282,   336,  1872,    30]])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(\"Ciao, come stai?\", return_tensors='pt')['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a458b725-63c1-49ae-8011-71a9196387b8",
   "metadata": {},
   "source": [
    "## <font color='red'>Exercise 2.2: Generating Text\n",
    "\n",
    "<font color='red'>There are a lot of ways we can, given a *prompt* in input, sample text from a GPT2 model. Instantiate a pre-trained `GPT2LMHeadModel` and use the [`generate()`](https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/text_generation#transformers.GenerationMixin.generate) method to generate text from a prompt.\n",
    "\n",
    "<font color='red'>**Note**: The default inference mode for GPT2 is *greedy* which might not results in satisfying generated text. Look at the `do_sample` and `temperature` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a637c368",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hello, how are you?\\n\\nI'm a little bit of a nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd\"]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GenerationConfig\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "configuration = GenerationConfig(max_length=40, pad_token_id=4300)\n",
    "\n",
    "inputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, generation_config=configuration)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2654fd9d-0731-496e-b6e9-b6d198a77899",
   "metadata": {},
   "source": [
    "In questo primo esperimento, si vede come la frase generata dal modello non abbia effettivamente senso compiuto.\n",
    "\n",
    "Proviamo ora a giocare con i parametri:\n",
    "- do_sample -> per passare da un decoding di tipo greedy a utilizzare effettivamente un sampling;\n",
    "- temperature -> parametro usato per manipolare la probabilità del token successivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d873a407",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello, how are you? I\\'m just sitting here and typing this.\\n\\n\"Hello, how are you?\"\\n\\n\"You\\'re fine. I\\'m doing okay.\"\\n\\n\"Yeah']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration2 = GenerationConfig(max_length=40, pad_token_id=4300, do_sample = True, temperature = 0.5)\n",
    "outputs2 = model.generate(**inputs, generation_config=configuration2)\n",
    "tokenizer.batch_decode(outputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2174c725-01ff-4e2d-ad00-62e1817412d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hello, how are you?\\n\\nI'm very happy.\\n\\nI'm very happy.\\n\\nI'm very happy.\\n\\nI'm very happy.\\n\\nI'm very happy\"]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration3 = GenerationConfig(max_length=40, pad_token_id=4300, do_sample = True, temperature = 0.3)\n",
    "outputs3 = model.generate(**inputs, generation_config=configuration3)\n",
    "tokenizer.batch_decode(outputs3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb00918c-7c99-4443-a453-5b21e1b83d7c",
   "metadata": {},
   "source": [
    "In tutti i casi, si vede come il modello non crei effettivamente una frase di senso compiuto e nei casi in cui prevale il decoding di tipo greedy ripeta le stesse parole all'infinito."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88229550",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <font color='red'>Exercise 3: Reusing Pre-trained LLMs (choose one)\n",
    "\n",
    "<font color='red'>Choose **one** of the following exercises (well, *at least* one). In each of these you are asked to adapt a pre-trained LLM (`GPT2Model` or `DistillBERT` are two good choices) to a new Natural Language Understanding task. A few comments:\n",
    "\n",
    "+ <font color='red'>Since GPT2 is a *autoregressive* model, there is no latent space aggregation at the last transformer layer (you get the same number of tokens out that you give in input). To use a pre-trained model for a classification or retrieval task, you should aggregate these tokens somehow (or opportunistically select *one* to use).\n",
    "\n",
    "+ <font color='red'>BERT models (including DistillBERT) have a special [CLS] token prepended to each latent representation in output from a self-attention block. You can directly use this as a representation for classification (or retrieval).\n",
    "\n",
    "+ <font color='red'><font color='red'>The first *two* exercises below can probably be done *without* any fine-tuning -- that is, just training a shallow MLP to classify or represent with the appropriate loss function.\n",
    "\n",
    "\n",
    "# <font color='red'>Exercise 3.2: Training a Question Answering Model (harder)\n",
    "\n",
    "<font color='red'>Peruse the [multiple choice question answering datasets on Hugging Face](https://huggingface.co/datasets?task_categories=task_categories:multiple-choice&sort=downloads). Chose a *moderately* sized one and train a model to answer contextualized multiple-choice questions. You *might* be able to avoid fine-tuning by training a simple model to *rank* the multiple choices (see margin ranking loss in Pytorch)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ac838a",
   "metadata": {},
   "source": [
    "Per svolgere questo esercizio si è studiato e approfondito il tutorial presente sul sito di Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc2d26f-bdff-43a9-b915-4d590db9bee3",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05533f3d",
   "metadata": {},
   "source": [
    "Per prima cosa, scarichiamo il dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e13c1a81",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cosmos_qa (C:/Users/Sofy/.cache/huggingface/datasets/cosmos_qa/regular/0.1.0/3e18538cbfdb2c04189b16642715f0f6da3e97ed5df0aadcec3641245b2cf157)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4404960405452bbe90c6ba357bb9ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cosmos_qa\", \"regular\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a10d1f2-fb37-4c46-98a3-f2eece6e66cf",
   "metadata": {},
   "source": [
    "E vediamo come è strutturato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3d2100c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '3Q9SPIIRWJKVQ8244310E8TUS6YWAC##34V1S5K3GTZMDUBNBIGY93FLDOB690##A1S1K7134S2VUC##Blog_1044056##q1_a1##3XU9MCX6VQQG7YPLCSAFDPQNH4GR20',\n",
       " 'context': \"Good Old War and person L : I saw both of these bands Wednesday night , and they both blew me away . seriously . Good Old War is acoustic and makes me smile . I really can not help but be happy when I listen to them ; I think it 's the fact that they seemed so happy themselves when they played .\",\n",
       " 'question': 'In the future , will this person go to see other bands play ?',\n",
       " 'answer0': 'None of the above choices .',\n",
       " 'answer1': 'This person likes music and likes to see the show , they will see other bands play .',\n",
       " 'answer2': 'This person only likes Good Old War and Person L , no other bands .',\n",
       " 'answer3': 'Other Bands is not on tour and this person can not see them .',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f0c386-b5b0-43eb-a93f-19166535ab38",
   "metadata": {},
   "source": [
    "Come si può osservare, ogni entry del dataset, presenta:\n",
    "- un **id**;\n",
    "- un **contesto** per la domanda;\n",
    "- la **domanda** a cui si dovrà rispondere;\n",
    "- quattro possibili **risposte**;\n",
    "- la **label** con indicato la risposta corretta.\n",
    "\n",
    "Per poter fare il fine-tuning di un modello già esistente è necessario svolgere prima una fase di preprocessing del dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23cb978",
   "metadata": {},
   "source": [
    "### Fase di preprocessing\n",
    "In questa fase di preprocessing, si vanno a creare i token del contesto unito alla domanda e alle quattro risposte possibili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de482bcc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "ending_names = [\"answer0\", \"answer1\", \"answer2\", \"answer3\"]\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    first_sentences = [[context] * 4 for context in examples[\"context\"]]\n",
    "    question_headers = examples[\"question\"]\n",
    "    second_sentences = [\n",
    "        [f\"{header} {examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers)\n",
    "    ]\n",
    "\n",
    "    first_sentences = sum(first_sentences, [])\n",
    "    second_sentences = sum(second_sentences, [])\n",
    "\n",
    "    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\n",
    "    return {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6848ec81-a25e-41a0-a765-014e677f3c44",
   "metadata": {},
   "source": [
    "Per eseguire il preprocessing sull'intero dataset, si una la funzione map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a10f4989",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Sofy\\.cache\\huggingface\\datasets\\cosmos_qa\\regular\\0.1.0\\3e18538cbfdb2c04189b16642715f0f6da3e97ed5df0aadcec3641245b2cf157\\cache-4fe95802ad28fd49.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Sofy\\.cache\\huggingface\\datasets\\cosmos_qa\\regular\\0.1.0\\3e18538cbfdb2c04189b16642715f0f6da3e97ed5df0aadcec3641245b2cf157\\cache-9debe1a1399a85e8.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Sofy\\.cache\\huggingface\\datasets\\cosmos_qa\\regular\\0.1.0\\3e18538cbfdb2c04189b16642715f0f6da3e97ed5df0aadcec3641245b2cf157\\cache-e6f2aeaf7bcecc5a.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b2507dd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'context', 'question', 'answer0', 'answer1', 'answer2', 'answer3', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 25262\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d972a35-1ea9-470b-95c3-4aaa7a75e876",
   "metadata": {},
   "source": [
    "Come si può vedere, il dataset risultante ha gli stessi campi del precedente dataset ma ha adesso anche due campi aggiuntivi:\n",
    "- il campo **inputs_ids** che contiene i token di context + question + answer;\n",
    "- il campo **attention_mask** che serve a indicare su quali token di *input_ids* porre l'attenzione.\n",
    "\n",
    "\n",
    "Per generare i batch per il fine tuning, si deve utilizzare un **Data Collator**: cioè uno strumento che inserisce il padding a ogni elemento del dataset per fare in modo che tutti gli elementi del batch abbiano la stessa grandezza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "727b4f25",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "import torch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = [feature.pop(label_name) for feature in features] if label_name in features[0].keys() else None\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0][\"input_ids\"])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        if labels is not None:\n",
    "            batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528958a2-49dd-456e-8696-ebc55f952575",
   "metadata": {},
   "source": [
    "### Accuracy Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826aee07-0dd3-4637-bde2-7f4405a908eb",
   "metadata": {},
   "source": [
    "Come metrica, ho deciso di calcolare l'accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "099ea0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Funzione per calcolare l'accuracy\n",
    "def accuracy(eval_pred):\n",
    "    y_pred, y_true = eval_pred\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    return {'accuracy': accuracy_score(y_true, y_pred)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c3cb74-9743-412b-9ba7-d49615c38dbb",
   "metadata": {},
   "source": [
    "### Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f59f6a-56f2-4a10-abf6-113981abe5ee",
   "metadata": {},
   "source": [
    "Usiamo adesso i Trainer di pytorch per fare il fine-tuning del modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c77315f0-c6ab-4c82-a594-01b71029ebe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultipleChoice: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForMultipleChoice.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b210ebf7-7bdf-41e7-99ee-1206d6ee352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
    "    compute_metrics=accuracy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a0eb0d4-55a4-4e4b-89e1-2a1927290a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sofy\\miniconda3\\envs\\DLA\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9474' max='9474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9474/9474 7:25:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.056300</td>\n",
       "      <td>1.005294</td>\n",
       "      <td>0.596315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.606500</td>\n",
       "      <td>1.358598</td>\n",
       "      <td>0.561809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.218000</td>\n",
       "      <td>2.640128</td>\n",
       "      <td>0.558794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9474, training_loss=0.6601521300947412, metrics={'train_runtime': 26724.5877, 'train_samples_per_second': 2.836, 'train_steps_per_second': 0.355, 'total_flos': 2.170273020618485e+16, 'train_loss': 0.6601521300947412, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25566958-fe8d-47fc-ab9d-1c1db8af83f6",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f541456b-3c38-4165-8d53-dd2e81d2410d",
   "metadata": {},
   "source": [
    "Il dataset scelto non presenta delle label valide all'interno del test_set. Di conseguenza, non possiamo calcolare l'accuracy sul test_set.\n",
    "\n",
    "Nota: ho guardato diversi dataset prima di scegliere questo e nessuno di loro aveva le label per tutti e tre i set (train, validation e test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67e72ca3-faaa-4059-8599-9ff877b2cbd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForMultipleChoice.from_pretrained('my_model/checkpoint-9474')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37551312-7871-4333-b165-7637168879b0",
   "metadata": {},
   "source": [
    "La funzione predict() del trainer di hugging face funziona anche senza che vengano passate le label. Ho quindi eliminato le label dal test_set prima di continuare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b798e95c-5d87-4ccc-8fe6-fb5c8cbd9cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'context', 'question', 'answer0', 'answer1', 'answer2', 'answer3', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 6963\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = tokenized_dataset['test']\n",
    "test_data = test_data.remove_columns(\"label\")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc46f9f0-c008-4738-ae14-491a9e8654ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0008c33e-e305-4e5d-a6f6-7b53c9873535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "test_result = trainer.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22e0f1a2-8319-41eb-b262-340f3a921b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-11.18111  ,  -6.029766 , -12.528319 , -12.3311825],\n",
       "       [ -8.765934 ,  -7.91253  ,   8.332372 ,   2.9878469],\n",
       "       [ -0.3738349,   3.801273 ,  11.359877 ,   3.3177714],\n",
       "       ...,\n",
       "       [ -6.252652 ,  -6.411227 ,  -7.7708454,   5.3823843],\n",
       "       [-12.507664 , -11.57149  ,  -6.6277285,  11.902547 ],\n",
       "       [  1.8697493,  -4.919432 ,   6.771773 ,  12.320783 ]],\n",
       "      dtype=float32), label_ids=None, metrics={'test_runtime': 78.7702, 'test_samples_per_second': 88.396, 'test_steps_per_second': 11.057})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27cad5ee-9163-4bc6-a5dd-92acbf44e2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = test_result.predictions.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cac18a5-06a7-4bd3-af99-05b7a629c083",
   "metadata": {},
   "source": [
    "Abbiamo ottenuto così la risposta predetta dal modello per ogni domanda nel test_set.\n",
    "\n",
    "Se ad esempio osserviamo la prima:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f822c85b-52b0-4a66-b29e-872a6ca574a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: HGH and steroid use is rampant in track , and , just like in most American professional sports , any incredible individual achievement is questioned immediately , which is a sad state of affairs . The fact that we , as a human population have to assume that someone is using steroids because of an outstanding feat is horrible . What about the people who actually train their hearts out to achieve the same success that the users are trying to achieve ? It is n't fair to the men and women that dedicate their lives to achieving glory the right way , hard work and determination .\n",
      "question: Why is HGH and steroid use rampant in track ?\n",
      "answer0: Because we have to assume that someone is using steroids because of an outstanding feat .\n",
      "answer1: None of the above choices .\n",
      "answer2: Because any incredible individual achievement is questioned immediately .\n",
      "answer3: Because it 's an American professional sport .\n"
     ]
    }
   ],
   "source": [
    "print(f\"context: {test_data[0]['context']}\")\n",
    "print(f\"question: {test_data[0]['question']}\")\n",
    "print(f\"answer0: {test_data[0]['answer0']}\")\n",
    "print(f\"answer1: {test_data[0]['answer1']}\")\n",
    "print(f\"answer2: {test_data[0]['answer2']}\")\n",
    "print(f\"answer3: {test_data[0]['answer3']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f53296-ac37-4ef6-89f4-7a3c2491367a",
   "metadata": {},
   "source": [
    "Abbiamo che la risposta predetta è:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd2098cd-7ab9-4814-a5ce-e1036473f6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted answer: answer number 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"predicted answer: answer number {predictions[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122b88f1-19a7-4569-bcb2-0f5878f852e3",
   "metadata": {},
   "source": [
    "Cioé:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77a5e816-2a6d-439e-8bc2-29dc92a3b1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'None of the above choices .'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]['answer1']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
